# Mod 3 Exposer une base de donn√©es relationnelle via une API REST et entrainement d'un mod√®le
###### Le `.venv` 


```bash
python -m venv .venv
```


* **Windows (PowerShell) :**
    ```bash
    .\.venv\Scripts\Activate.ps1
    ```
* **Windows (CMD) :**
    ```bash
    .\.venv\Scripts\activate.bat
    ```
* **macOS / Linux :**
    ```bash
    source .venv/bin/activate
    ```


###### Le `requirements.txt`


Assure-toi que ton `.venv` est activ√©, puis :

```bash
pip install -r requirements.txt
```


#### üó∫Ô∏è Architecture : O√π va quoi dans notre petit monde ? üó∫Ô∏è


.
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ data-all-684bf775c031b265646213.csv
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ models.py
‚îÇ   ‚îú‚îÄ‚îÄ model_2024_08.pkl
‚îÇ   ‚îî‚îÄ‚îÄ preprocessor.pkl
‚îú‚îÄ‚îÄ figures/
‚îÇ   ‚îú‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py
‚îÇ   ‚îú‚îÄ‚îÄ preprocess.py
‚îÇ   ‚îî‚îÄ‚îÄ print_draw.py
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ app.py  => root de front
‚îú‚îÄ‚îÄ mlFlow_api.py => API FastAPI 
‚îî‚îÄ‚îÄ requirements.txt
```

###### `data/`
* `data-all-684bf775c031b265646213.csv` : Les donn√©es de base √† traiter.

###### `figures/` (L'analyste visuelle)
Sauvegarde des images des courbes de co√ªt et autres graphiques pour visualiser les performances de notre mod√®le.

###### `models/` (Le garage √† cerveaux)
Ce dossier, c'est notre caverne d'Ali Baba des cerveaux artificiels.
* `models.py` : Les plans de nos futurs cyborgs... euh, de nos mod√®les. C'est ici que l'on d√©finit l'architecture de nos NN et autres merveilles.
* `model_2024_08.pkl` : Une version sauvegard√©e de notre mod√®le. On l'a encapsul√© pour qu'il ne s'√©chappe pas et ne domine pas le monde... pas encore.
* `preprocessor.pkl` : L'outil magique qui pr√©pare les donn√©es avant de les donner √† manger au mod√®le. Sans lui, c'est l'indigestion assur√©e !

###### `modules/` (La bo√Æte √† outils de MacGyver)
Ce sont nos couteaux suisses du code. Chaque fichier est un expert dans son domaine.
* `evaluate.py` : Le juge impitoyable qui dit si notre mod√®le est un g√©nie ou un cancre.
* `preprocess.py` : Le chef cuisinier des donn√©es. Il les nettoie, les coupe, les assaisonne pour qu'elles soient parfaites pour notre IA.
* `print_draw.py` : L'artiste du groupe. Il transforme nos chiffres barbares en beaux graphiques pour que m√™me ta grand-m√®re puisse comprendre (enfin, presque).

---

On esp√®re que cette petite vir√©e dans notre projet t'a plu. N'h√©site pas √† jeter un ≈ìil au `main.py` pour lancer le grand spectacle !

*Fait avec amour, code et une bonne dose de caf√©ine (et un peu de folie).*


# TD => GOGOGO
## setup

- ### G√©n√©ration requirements.txt √† chaque installation de module
```bash
pip freeze > requirements.txt
```

- ### Installations des requis sqlAlchemy: 
```bash
pip install psycopg2-binary
pip install pydantic-sqlalchemy
```

- ### Installations des requis loguru: 
```bash
pip install loguru
```

- ### Installations des requis FastAPI/Streamlit: 
```bash
pip install nltk fastapi streamlit uvicorn requests pydantic
```
- #### Pour lancer le serveur MLflow :
```bash
uvicorn mlFlow_api:app --host 127.0.0.1 --port 8000 --reload
```
- #### Description des routes de l'API FastAPI :
[GET /docs](http://127.0.0.1:8000/docs#/)


- ### Installation des biblioth√®ques pour les tests unitaires: 
```bash
pip install pytest httpx
pytest test_predict_api.py
```

- ### Installations des requis pour MLflow : 
  > **mlFlow**
  MlFlow est un outil de gestion des exp√©riences de machine learning. Il permet de suivre les exp√©riences, de g√©rer les mod√®les et de visualiser les r√©sultats.
```bash
pip install mlflow scikit-learn pandas matplotlib
```

# Pour lancer le serveur MLflow :
```bash
mlflow ui
```

## Streamlit
lancer le serveur Streamlit pour l'interface utilisateur :
```bash
streamlit run streamlit_app.py
```
### Pour acc√©der au front (root + pages entrainnement et prediction :)
[Streamlit Front](http://localhost:8501)


## D√©roul√© du travail
Cr√©ation d'un script pour g√©n√©rer 3 entrainements et les stocker sur MLflow : 
Les models cr√©√© sont stock√©s dans le dossier `models/` et pictures du drawloss sont stock√©s dans le dossier `figures/`.


- J'ai mis en place :
  - Un script pour lancer 5 entrainements sur les anciennes donn√©es et 5 sur les nouvelles donn√©es.
  - le suivi des performances de chaque entrainement dans MLflow. (possibilit√© de stocker ou pas les mod√®les et les graph de loss)
  - les tests unitaires avec pytest
  - le loging des performances avec `loguru` et un setup simplifi√© pour le logger.
  - les images des couts stock√©s dans le dossier `figures/`.
  - Une route `/predict` pour faire des pr√©dictions sur des donn√©es envoy√©es via questionnaire *Streamlit*.
  - Une route `/retrain` pour r√©entrainer le mod√®le (in progress).
  - Containerisation avec docker File puis docker-compose 
  - Stockage de l'id du model en local dans un fichier pour le retrouver depuis docker



**Docker : Build & Run**

Pour builder l‚Äôimage Docker manuellement‚ÄØ:
```bash
docker build -t mlflow-app .
```
Puis lancer le conteneur (tous services dans le m√™me conteneur)‚ÄØ:
```bash
docker run -p 8000:8000 -p 8501:8501 -p 5000:5000 mlflow-app
```

- FastAPI : http://localhost:8000/docs
- Streamlit : http://localhost:8501
- MLflow UI : http://localhost:5000

**Avec Docker Compose (meilleur approche)**

Pour builder et lancer tous les services (API, Streamlit, MLflow) dans des conteneurs s√©par√©s‚ÄØ:
```bash
docker compose up --build
```

Pour tout arr√™ter proprement‚ÄØ:
```bash
docker compose down
```


```powershell
wsl --unregister docker-desktop
wsl --unregister docker-desktop-data
```

## setup docker postgresql
```powershell
docker compose -f docker-compose-postgres.yml up -d
```

## Etape cl√©s du projet
- J'ai cr√©√© un nouveau projet √† partir des modules pr√©c√©dent : FastAPI / MLFlow / Streamlit/ LOGURU / Docker / 
- Le jeu de donn√©es est stock√© dans le dossier `data/` sous le nom `data-all-684bf775c031b265646213.csv`.
- J'ai suivi le module sur la prise en main de SQL Alchimy et la gestion ORM
- Le notebook est disponible dans le dossier `notebooks/` sous le nom `SQLAlchemy_exploration.ipynb`.
- Utilisation du notebook cr√©√© lors du module 2 pour analyser et cr√©er un jeu de donn√©es propre (CSV).
- R√©utilisation du notebook cr√©√© lors du module 2 pour analyser le jeu de donn√©es, comprendre les donn√©es et les nettoyer.
- Le notebook est disponible dans le dossier `notebooks/` sous le nom `ethique_data_cleaning.ipynb`.


- J'ai cr√©√© un script `python alchemy_api.py clean_dataset` pour nettoyer le jeu de donn√©es et le stocker dans le dossier `data/`.
- 
- J'ai cr√©√© une DB PostgreSQL containeris√© que j'ai rempli avec les valeurs  j'ai cr√©√© un CRUD sur la ressource Client. J'ai √©galement cr√©√© un pr√©processeur et j'ai cr√©√© un mod√®le que j'ai entra√Æn√© avec les donn√©es pr√©process√©es de la DB.
- J'ai chang√© le preprocesseur pour s'adapter aux nouvelles colonnes du jeu de donn√©es.
Quelles difficult√©s j‚Äôai rencontr√©es dans la journ√©e ? 
- l'apprentissage des nouvelles librairies et leur impl√©mentation dans une architecture relativement propre, la r√©utilisation d"outils d√©ja vu (MLFlow ...). 
Qu‚Äôest-ce que j‚Äôai appris ? 
- J'ai appris √† utiliser l'ORM SQLAlchemy et √† g√©rer une DB avec.
- J'ai consolid√© les outils √† ma disposition pour cr√©er une API REST et un mod√®le de machine learning : 
  - Simplification de l'initialisation 
  ```python 
    ## Base initialisation for Loguru and FastAPI
    from myapp_base import setup_loguru, app, Request, HTTPException
    logger = setup_loguru("logs/alchemy_api.log")
  ```

- Je suis repass√© sur sqllite en db car trop de soucis avec postgres

- J'ai cr√©√© des scripts utilitaires pour injecter les donn√©es `python inject_data.py init`

- Ajout du module pydantic-sqlalchemy pour g√©rer les changements de mod√®les et rendre + dynamique pydantic

> **Note :** A quoi sert le mod√®le ?

## Documentation du flux de donn√©es et √©valuation √©thique

### Flux de donn√©es et transformations

- Les donn√©es brutes sont import√©es depuis un fichier CSV (`data-all-complete-684bf9cd92797851623245.csv`).
- Un nettoyage √©thique est r√©alis√© dans le notebook `ethique_data_cleaning_complete.ipynb`¬†:  
  - Suppression des colonnes sensibles ou non conformes (nom, pr√©nom, sexe, nationalit√©, orientation sexuelle, date de cr√©ation de compte).
  - Remplissage des valeurs manquantes pour certaines colonnes (`loyer_mensuel` par la moyenne, `situation_familiale` par la modalit√© la plus fr√©quente).
  - Filtrage des valeurs aberrantes (poids, nb_enfants, quotient_caf, loyer_mensuel).
  - Suppression des colonnes avec trop de valeurs manquantes (`score_credit`, `historique_credits`).
- Le dataset nettoy√© est sauvegard√© sous `df_data_all_complete_cleaned.csv` puis inject√© dans la base via le script `inject_data.py`.

### Sch√©ma de la base de donn√©es

- Table principale¬†: `loandatas`
- Colonnes¬†:  
  `id`, `age`, `taille`, `poids`, `nb_enfants`, `quotient_caf`, `sport_licence`, `niveau_etude`, `region`, `smoker`, `revenu_estime_mois`, `situation_familiale`, `risque_personnel`, `loyer_mensuel`, `montant_pret`
- Le sch√©ma est versionn√© et document√© via les migrations Alembic.

### √âvaluation √©thique

- Les colonnes √† risque de discrimination ou d‚Äôidentification ont √©t√© supprim√©es.
- Les choix de remplissage et de filtrage sont document√©s dans le notebook.
- Les risques de biais restants (ex¬†: sur la r√©gion, le niveau d‚Äô√©tude, ou le remplissage par la moyenne) sont identifi√©s et √† surveiller lors de l‚Äôutilisation des donn√©es.

---




