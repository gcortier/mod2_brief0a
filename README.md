# Mod 3 Exposer une base de donn√©es relationnelle via une API REST et entrainement d'un mod√®le
###### Le `.venv` 


```bash
python -m venv .venv
```


* **Windows (PowerShell) :**
    ```bash
    .\.venv\Scripts\Activate.ps1
    ```
* **Windows (CMD) :**
    ```bash
    .\.venv\Scripts\activate.bat
    ```
* **macOS / Linux :**
    ```bash
    source .venv/bin/activate
    ```


###### Le `requirements.txt`


Assure-toi que ton `.venv` est activ√©, puis :

```bash
pip install -r requirements.txt
```


#### üó∫Ô∏è Architecture : O√π va quoi dans notre petit monde ? üó∫Ô∏è


.
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ data-all-684bf775c031b265646213.csv
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ models.py
‚îÇ   ‚îú‚îÄ‚îÄ model_2024_08.pkl
‚îÇ   ‚îî‚îÄ‚îÄ preprocessor.pkl
‚îú‚îÄ‚îÄ figures/
‚îÇ   ‚îú‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py
‚îÇ   ‚îú‚îÄ‚îÄ preprocess.py
‚îÇ   ‚îî‚îÄ‚îÄ print_draw.py
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ app.py  => root de front
‚îú‚îÄ‚îÄ mlFlow_api.py => API FastAPI 
‚îî‚îÄ‚îÄ requirements.txt
```

###### `data/` (Le garde-manger du projet)
Ici, c'est l√† que nos pr√©cieuses donn√©es vivent.
* `df_new.csv` : Les donn√©es fra√Æches du jour, pr√™tes √† √™tre d√©vor√©es par notre IA.
* `df_old.csv` : Les classiques, les v√©t√©rans, ceux qui ont tout vu. On les garde par nostalgie (et pour la r√©trospective).

###### `figures/` (L'analyste visuelle)
Sauvegarde des images des courbes de co√ªt et autres graphiques pour visualiser les performances de notre mod√®le.

###### `models/` (Le garage √† cerveaux)
Ce dossier, c'est notre caverne d'Ali Baba des cerveaux artificiels.
* `models.py` : Les plans de nos futurs cyborgs... euh, de nos mod√®les. C'est ici que l'on d√©finit l'architecture de nos NN et autres merveilles.
* `model_2024_08.pkl` : Une version sauvegard√©e de notre mod√®le. On l'a encapsul√© pour qu'il ne s'√©chappe pas et ne domine pas le monde... pas encore.
* `preprocessor.pkl` : L'outil magique qui pr√©pare les donn√©es avant de les donner √† manger au mod√®le. Sans lui, c'est l'indigestion assur√©e !

###### `modules/` (La bo√Æte √† outils de MacGyver)
Ce sont nos couteaux suisses du code. Chaque fichier est un expert dans son domaine.
* `evaluate.py` : Le juge impitoyable qui dit si notre mod√®le est un g√©nie ou un cancre.
* `preprocess.py` : Le chef cuisinier des donn√©es. Il les nettoie, les coupe, les assaisonne pour qu'elles soient parfaites pour notre IA.
* `print_draw.py` : L'artiste du groupe. Il transforme nos chiffres barbares en beaux graphiques pour que m√™me ta grand-m√®re puisse comprendre (enfin, presque).

---

On esp√®re que cette petite vir√©e dans notre projet t'a plu. N'h√©site pas √† jeter un ≈ìil au `main.py` pour lancer le grand spectacle !

*Fait avec amour, code et une bonne dose de caf√©ine (et un peu de folie).*


# TD => GOGOGO
## setup

- ### G√©n√©ration requirements.txt √† chaque installation de module
```bash
pip freeze > requirements.txt
```

- ### Installations des requis sqlAlchemy: 
```bash
pip install psycopg2-binary
```

- ### Installations des requis loguru: 
```bash
pip install loguru
```

- ### Installations des requis FastAPI/Streamlit: 
```bash
pip install nltk fastapi streamlit uvicorn requests pydantic
```
- #### Pour lancer le serveur MLflow :
```bash
uvicorn mlFlow_api:app --host 127.0.0.1 --port 8000 --reload
```
- #### Description des routes de l'API FastAPI :
[GET /docs](http://127.0.0.1:8000/docs#/)


- ### Installation des biblioth√®ques pour les tests unitaires: 
```bash
pip install pytest httpx
pytest test_predict_api.py
```

- ### Installations des requis pour MLflow : 
  > **mlFlow**
  MlFlow est un outil de gestion des exp√©riences de machine learning. Il permet de suivre les exp√©riences, de g√©rer les mod√®les et de visualiser les r√©sultats.
```bash
pip install mlflow scikit-learn pandas matplotlib
```

# Pour lancer le serveur MLflow :
```bash
mlflow ui
```

## Streamlit
lancer le serveur Streamlit pour l'interface utilisateur :
```bash
streamlit run streamlit_app.py
```
### Pour acc√©der au front (root + pages entrainnement et prediction :)
[Streamlit Front](http://localhost:8501)


## D√©roul√© du travail
Cr√©ation d'un script pour g√©n√©rer 3 entrainements et les stocker sur MLflow : 
Les models cr√©√© sont stock√©s dans le dossier `models/` et pictures du drawloss sont stock√©s dans le dossier `figures/`.


- J'ai mis en place :
  - Un script pour lancer 5 entrainements sur les anciennes donn√©es et 5 sur les nouvelles donn√©es.
  - le suivi des performances de chaque entrainement dans MLflow. (possibilit√© de stocker ou pas les mod√®les et les graph de loss)
  - les tests unitaires avec pytest
  - le loging des performances avec `loguru` et un setup simplifi√© pour le logger.
  - les images des couts stock√©s dans le dossier `figures/`.
  - Une route `/predict` pour faire des pr√©dictions sur des donn√©es envoy√©es via questionnaire *Streamlit*.
  - Une route `/retrain` pour r√©entrainer le mod√®le (in progress).
  - Containerisation avec docker File puis docker-compose 
  - Stockage de l'id du model en local dans un fichier pour le retrouver depuis docker



**Docker : Build & Run**

Pour builder l‚Äôimage Docker manuellement‚ÄØ:
```bash
docker build -t mlflow-app .
```
Puis lancer le conteneur (tous services dans le m√™me conteneur)‚ÄØ:
```bash
docker run -p 8000:8000 -p 8501:8501 -p 5000:5000 mlflow-app
```

- FastAPI : http://localhost:8000/docs
- Streamlit : http://localhost:8501
- MLflow UI : http://localhost:5000

**Avec Docker Compose (meilleur approche)**

Pour builder et lancer tous les services (API, Streamlit, MLflow) dans des conteneurs s√©par√©s‚ÄØ:
```bash
docker compose up --build
```

Pour tout arr√™ter proprement‚ÄØ:
```bash
docker compose down
```


```powershell
wsl --unregister docker-desktop
wsl --unregister docker-desktop-data
```

## setup docker postgresql
```powershell
docker compose -f docker-compose-postgres.yml up -d
```

## Etape clen du jeu de donn√©es
- Utilisation du notebook cr√©√© lors du module 2 pour cr√©er un jeu de donn√©es propre.
- Le jeu de donn√©es est stock√© dans le dossier `data/` sous le nom `data-all-684bf775c031b265646213.csv`.
- Le notebook est disponible dans le dossier `notebooks/` sous le nom `ethique_data_cleaning.ipynb`.
